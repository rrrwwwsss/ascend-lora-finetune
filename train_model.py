import os
import torch
device = "npu:0"  # æˆ–è€…é€šè¿‡å…¶ä»–æ–¹å¼åŠ¨æ€ç¡®å®š
# torch.npu.set_device("npu:7")
import pandas as pd

from datasets import Dataset
from modelscope import snapshot_download, AutoTokenizer
# from swanlab.integration.transformers import SwanLabCallback
from qwen_vl_utils import process_vision_info

from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    Qwen2VLForConditionalGeneration,
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
)
# import swanlab
import json
import torch_npu

# swanlab.login(api_key="oSr42Kdg1W8ZMcQWMAbbj", save=True)
print(torch.npu.device_count())  # åº”è¯¥è¾“å‡º 1
print(torch.npu.current_device())  # åº”è¯¥è¾“å‡º 0


def process_func(example):
    device = 'npu:0'
    """
    å°†æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†
    """
    MAX_LENGTH = 8192
    output_content = example["model_result"]
    prompt = example["prompt_text"]
    file_path ="./train_data/pic_pack/" + example["image_path"].replace("\\", "/")  # è·å–å›¾åƒè·¯å¾„
    messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": f"{file_path}",
                    "resized_height": 280,
                    "resized_width": 280,
                },
                {"type": "text", "text": prompt},
            ],
        }
    ]
    text = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )  # è·å–æ–‡æœ¬
    image_inputs, video_inputs = process_vision_info(messages)  # è·å–æ•°æ®æ•°æ®ï¼ˆé¢„å¤„ç†è¿‡ï¼‰
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = {key: value.to(device) for key, value in inputs.items()}  # tensor -> list,ä¸ºäº†æ–¹ä¾¿æ‹¼æ¥
    instruction = inputs

    response = tokenizer(f"{output_content}", add_special_tokens=False)

    input_ids = (
            instruction["input_ids"][0].tolist() + response["input_ids"] + [tokenizer.pad_token_id]
    )

    attention_mask = instruction["attention_mask"][0].tolist() + response["attention_mask"] + [1]
    labels = (
            [-100] * len(instruction["input_ids"][0])
            + response["input_ids"]
            + [tokenizer.pad_token_id]
    )
    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­
        input_ids = input_ids[:MAX_LENGTH]
        attention_mask = attention_mask[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]

    input_ids = torch.tensor(input_ids).to(device)
    attention_mask = torch.tensor(attention_mask).to(device)
    labels = torch.tensor(labels).to(device)
    print("è®¾å¤‡ï¼š", device)
    print(f"input_ids è®¾å¤‡: {input_ids.device}")
    print(f"attention_mask è®¾å¤‡: {attention_mask.device}")
    print(f"labels è®¾å¤‡: {labels.device}")
    print(f"pixel_values è®¾å¤‡: {inputs['pixel_values'].device}")
    print(f"image_grid_thw è®¾å¤‡: {inputs['image_grid_thw'].device}")
    inputs['pixel_values'] = torch.tensor(inputs['pixel_values'])
    inputs['image_grid_thw'] = torch.tensor(inputs['image_grid_thw']).squeeze(0)  # ç”±ï¼ˆ1,h,w)å˜æ¢ä¸ºï¼ˆh,wï¼‰
    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels,
            "pixel_values": inputs['pixel_values'], "image_grid_thw": inputs['image_grid_thw']}


# def predict(messages, model):
#     # å‡†å¤‡æ¨ç†
#     text = processor.apply_chat_template(
#         messages, tokenize=False, add_generation_prompt=True
#     )
#     image_inputs, video_inputs = process_vision_info(messages)
#     inputs = processor(
#         text=[text],
#         images=image_inputs,
#         videos=video_inputs,
#         padding=True,
#         return_tensors="pt",
#     )
#     # âœ… æ­£ç¡®æ–¹å¼ï¼šéå†å­—å…¸ï¼ŒæŠŠæ¯ä¸ª tensor ç§»åˆ° npu
#     inputs = {k: v.to("npu:7") for k, v in inputs.items()}
#
#     # ç”Ÿæˆè¾“å‡º
#     generated_ids = model.generate(**inputs, max_new_tokens=128)
#     generated_ids_trimmed = [
#         out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
#     ]
#     output_text = processor.batch_decode(
#         generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
#     )
#
#     return output_text[0]
model_name = "/data02/Qwen2.5-VL-7B-Instruct"
# ä½¿ç”¨TransformersåŠ è½½æ¨¡å‹æƒé‡
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
processor = AutoProcessor.from_pretrained(model_name,trust_remote_code=True,
    use_fast=False  # é¿å… fast tokenizer è­¦å‘Š
)
# trust_remote_code=True å¯ä»¥æ‹‰å–è¿œç¨‹ä»£ç   .cuda()PyTorch æŠŠæ•´ä¸ªæ¨¡å‹æ”¾åˆ°å•ä¸ª GPU
#model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_name,
#        torch_dtype=torch.bfloat16, trust_remote_code=True,device_map={"": "npu:7"})
from transformers import AutoModelForVision2Seq
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    device_map={"": device},
    torch_dtype=torch.float16,
    trust_remote_code=True,      # å¿…é¡»ï¼
    local_files_only=True        # å¼ºåˆ¶æœ¬åœ°åŠ è½½
)
# âœ… 2. æ‰‹åŠ¨è¿ç§»æ•´ä¸ªæ¨¡å‹
# model = model.to("npu:7")
device = list(model.parameters())[0].device
# âœ… å¼ºåˆ¶æŠŠ embedding ç§»åŠ¨åˆ° npu:7
model.get_input_embeddings().to(device)
if hasattr(model, "get_output_embeddings") and model.get_output_embeddings() is not None:
    model.get_output_embeddings().to(device)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•

print("æ¨¡å‹æ‰€åœ¨npu")
#print(model.hf_device_map)
print(next(model.parameters()).device)
#for name, param in model.named_parameters():
#    print(name, param.device)
# å¤„ç†æ•°æ®é›†ï¼šè¯»å–jsonæ–‡ä»¶
# æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä¿å­˜ä¸ºdata_vl_train.jsonå’Œdata_vl_test.json
import csv
from datasets import Dataset

csv_path = "./train_data/train_data.csv"

data = []
with open(csv_path, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        data.append(row)

train_dataset = Dataset.from_list(data)
# æ•°æ®é¢„å¤„ç†
train_dataset = train_dataset.map(process_func)

print("é¢„å¤„ç†åçš„æ•°æ®ï¼š",train_dataset)
# é…ç½®LoRA
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, #å®šä¹‰ä»»åŠ¡ç±»å‹ã€‚
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], #æŒ‡å®šå“ªäº›å±‚çš„æ¨¡å—å°†ä½¿ç”¨ LoRA è¿›è¡Œå¾®è°ƒã€‚
    inference_mode=False,  # è®­ç»ƒæ¨¡å¼ï¼Œå³ LoRA ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°å¯¹åº”å±‚çš„æƒé‡ï¼›å¦‚æœè®¾ç½®ä¸º Trueï¼Œåˆ™è¿›å…¥ æ¨ç†æ¨¡å¼ï¼Œæ­¤æ—¶ä¸ä¼šæ›´æ–°å‚æ•°ï¼Œåªä¼šä½¿ç”¨åœ¨è®­ç»ƒä¸­å·²ç»å­¦ä¹ åˆ°çš„å‚æ•°è¿›è¡Œæ¨ç†ã€‚
    r=64,  # Lora ç§©
    lora_alpha=16,  # æ§åˆ¶LoRAé€‚åº”éƒ¨åˆ†ä¸åŸå§‹æ¨¡å‹çš„æƒé‡èåˆæ¯”ä¾‹ã€‚è¾ƒå¤§çš„ lora_alphaä¼šå¢åŠ  LoRA éƒ¨åˆ†çš„å½±å“åŠ›ï¼Œè¾ƒå°çš„ lora_alpha åˆ™ä¼šè®© LoRA çš„å½±å“å‡å¼±ã€‚
    lora_dropout=0.05,  # Dropout æ¯”ä¾‹ï¼ŒDropout å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯åœ¨å¾®è°ƒæ—¶ï¼Œé€‚å½“çš„ dropout å¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚0.05 è¡¨ç¤º 5% çš„æ¦‚ç‡å°†è¢«â€œä¸¢å¼ƒâ€æˆ–å±è”½ã€‚
    bias="none",#bias="none" è¡¨ç¤º LoRA ä¸ä¼šå¯¹å±‚çš„åç½®é¡¹ï¼ˆbiasï¼‰è¿›è¡Œä¿®æ”¹ã€‚
)

# è·å–LoRAæ¨¡å‹
peft_model = get_peft_model(model, config)
# æ¨¡å‹åŠ è½½å®Œä¹‹åï¼Œç«‹åˆ»æ·»åŠ è¿™ä¸€è¡Œï¼š
# peft_model = peft_model.to(device)
for name, param in peft_model.named_parameters():
  if "lora" in name.lower():
       print(name, param.device)
# å®šä¹‰è¾“å‡ºç›®å½•

for name, param in peft_model.named_parameters():
    if str(param.device) != "npu:7":
        print(f"å‚æ•° {name} ä¸åœ¨ npu:7ï¼Œæ­£åœ¨è¿ç§»...")
        param.data = param.data.to(device)
output_dir = "./models/output/qwen2_vl_finetuned"

# âœ… é€’å½’åˆ›å»ºç›®å½•ï¼ˆå¦‚æœå·²å­˜åœ¨ï¼Œä¸æŠ¥é”™ï¼‰
os.makedirs(output_dir, exist_ok=True)
# é…ç½®è®­ç»ƒå‚æ•°
args = TrainingArguments(
    output_dir=output_dir,  #å­˜å‚¨æœ€ç»ˆçš„å¾®è°ƒæ¨¡å‹
    per_device_train_batch_size=4, #å•ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰ä¸Šä¸€æ¬¡è¿­ä»£çš„ batch æ•°é‡ï¼Œè¾ƒå¤§çš„æ‰¹æ¬¡å¯ä»¥æ›´å¥½åˆ©ç”¨GPUï¼Œä½†ä¼šå ç”¨æ›´å¤šæ˜¾å­˜
    gradient_accumulation_steps=4, #æ¢¯åº¦ç§¯ç´¯çš„æ­¥éª¤æ•°ï¼Œå¦‚æœæ˜¾å­˜ä¸å¤Ÿå¯ä»¥è®¾ç½®å¤šä¸ªå°çš„æ‰¹æ¬¡ç´¯è®¡æ¢¯åº¦ï¼Œè¿™ä¸ªè¡¨ç¤ºæ¢¯åº¦åœ¨4ä¸ªå°æ­¥éª¤åæ‰è¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­æ›´æ–°
    logging_steps=10, #æ¯ä¸ª10æ­¥ä¸€æ¬¡æ—¥å¿—è®°å½•
    logging_first_step=True,#æ˜¯å¦åœ¨è®­ç»ƒçš„ç¬¬ä¸€æ­¥åè¿›è¡Œæ—¥å¿—è®°å½•ã€‚
    num_train_epochs=2,#è®­ç»ƒçš„æ€»è½®æ•°ï¼Œå°†æ•°æ®é›†è®­ç»ƒå¤šå°‘é
    save_steps=100,#æ¯ 100 ä¸ªæ­¥éª¤ä¿å­˜ä¸€æ¬¡æ¨¡å‹ã€‚
    learning_rate=1e-4,#å­¦ä¹ ç‡ï¼šæ§åˆ¶æ¯æ¬¡å‚æ•°æ›´æ–°çš„æ­¥é•¿
    save_on_each_node=True, #åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šä¿å­˜æ¨¡å‹ã€‚å¤šç”¨äºåˆ†å¸ƒå¼è®­ç»ƒ
    gradient_checkpointing=True,#å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥å‡å°‘æ˜¾å­˜çš„ä½¿ç”¨ï¼Œä½†è®¡ç®—è¿‡ç¨‹ä¼šå˜æ…¢
    report_to="none",#æ˜¯å¦å°†è®­ç»ƒæ—¥å¿—æŠ¥å‘Šåˆ°å¤–éƒ¨ç³»ç»Ÿ
)

from transformers import DataCollatorForSeq2Seq

class NPUDataCollator(DataCollatorForSeq2Seq):
    def __init__(self, tokenizer, padding=True, model=None):
        super().__init__(tokenizer, padding=padding)
        self.model = model  # ä¼ å…¥æ¨¡å‹ä»¥è·å–å…¶ä¸»è®¾å¤‡

    def __call__(self, features):
        # å…ˆç”¨çˆ¶ç±»æ–¹æ³•è¿›è¡Œå¡«å……
        batch = super().__call__(features)
        # å°†æ‰€æœ‰å¼ é‡ç§»åŠ¨åˆ°æ¨¡å‹çš„ä¸»è®¾å¤‡
        if self.model is not None:
            device = self.model.device
            print(device)
            batch = {k: v.to(device) if hasattr(v, 'to') else v for k, v in batch.items()}
        return batch

# ä½¿ç”¨æ—¶
data_collator = NPUDataCollator(tokenizer=tokenizer, padding=True, model=peft_model)

# é…ç½®Trainer
trainer = Trainer(
    model=peft_model,
    args=args,
    train_dataset=train_dataset,
    # data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    # callbacks=[swanlab_callback],
    data_collator=data_collator,  # ä½¿ç”¨è‡ªå®šä¹‰çš„ collator
)
# è·å–è®­ç»ƒæ•°æ®åŠ è½½å™¨
train_dataloader = trainer.get_train_dataloader()

# å–ä¸€ä¸ª batch å¹¶æ‰“å°è®¾å¤‡ä¿¡æ¯
for step, batch in enumerate(train_dataloader):
    print("\nğŸ” å½“å‰ Batch ä¸­å„å¼ é‡æ‰€åœ¨çš„è®¾å¤‡ï¼š")
    for k, v in batch.items():
        if hasattr(v, "device"):
            print(f"  {k}: {v.device} (shape: {v.shape})")
        else:
            print(f"  {k}: {type(v)} (æ—  device å±æ€§)")
    break  # åªçœ‹ç¬¬ä¸€ä¸ª batch
print("å¼€å§‹è®­ç»ƒ...")
# å¼€å¯æ¨¡å‹è®­ç»ƒ
trainer.train()

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ä¸åˆ†è¯å™¨
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
print("å¾®è°ƒæ¨¡å‹ä¿å­˜æˆåŠŸï¼")


